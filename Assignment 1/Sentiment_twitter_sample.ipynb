{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import twitter_samples\n",
    "import pprint\n",
    "\n",
    "# Load dependencies\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import ConnectionFailure\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Location MongoDB:\n",
    "mongo_host = None\n",
    "# Name collection\n",
    "client_name = \"fundamentals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection\n",
    "try:\n",
    "    client = MongoClient(mongo_host)\n",
    "    client.admin.command('ismaster')\n",
    "    db = client[client_name] \n",
    "    twitter_db = db.twitter\n",
    "    \n",
    "except ConnectionFailure:\n",
    "    print(\"Connection to MongoDB server could not be established\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sanitizer(object):\n",
    "    \"\"\"\n",
    "    Class for sanitizing twitter messages.\n",
    "\n",
    "    @example        ```\n",
    "                    sanitizer = Sanitizer()\n",
    "\n",
    "                    with open('mytextfile.txt', 'r') as f:\n",
    "                        for line in f:\n",
    "                            print(sanitizer.sanitize(line))\n",
    "                    ```\n",
    "\n",
    "    @dependencies   nltk, string, re\n",
    "    @author         Tycho Atsma <tycho.atsma@student.uva.nl>\n",
    "    @file           Sanitizer.py\n",
    "    @documentation  public\n",
    "    @copyright      University of Amsterdam\n",
    "    \"\"\"\n",
    "    punctuation = string.punctuation\n",
    "    stopwords = nltk.corpus.stopwords\n",
    "    Lemmatizer = nltk.stem.WordNetLemmatizer\n",
    "    Tokenizer = nltk.tokenize.TweetTokenizer\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \"\"\"\n",
    "        self.punctuation_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        self.lemmatizer = self.Lemmatizer()\n",
    "        self.tokenizer = self.Tokenizer()\n",
    "\n",
    "    def remove_punctuation(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove punctuation from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        return message.translate(self.punctuation_table).strip()\n",
    "\n",
    "    def remove_stopwords(self, message, language=\"english\"):\n",
    "        \"\"\"\n",
    "        Method to remove stopwords from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @param  string  Language of the stopwords (default: english).\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        stops = self.stopwords.words(language)\n",
    "        tokens = message.split()\n",
    "        return \" \".join([token for token in tokens if token not in stops]).strip()\n",
    "\n",
    "    def remove_links(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove links from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        # source: https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url\n",
    "        pattern = r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n",
    "        return re.sub(pattern, \"\", message).strip()\n",
    "\n",
    "    def remove_usertags(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove usertags from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        pattern = r\"(\\@\\w*)\"\n",
    "        return re.sub(pattern, \"\", message).strip()\n",
    "\n",
    "    def remove_hashtags(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove hashtags from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        pattern = r\"(\\#\\w*)\"\n",
    "        return re.sub(pattern, \"\", message).strip()\n",
    "\n",
    "    def lemmatize(self, message):\n",
    "        \"\"\"\n",
    "        Method to lemmatize a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        tokens = message.split()\n",
    "        return \" \".join([self.lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "    def tokenize(self, message):\n",
    "        \"\"\"\n",
    "        Method to tokenize a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        return self.tokenizer.tokenize(message)\n",
    "\n",
    "    def sanitize(self, message):\n",
    "        \"\"\"\n",
    "        Method to sanitize a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        # 1. we need to normalize the message\n",
    "        message = unicodedata.normalize(\"NFC\", message.lower().encode('utf8').decode('utf8'))\n",
    "\n",
    "        # 2. we need to get rid of specific types of tokens\n",
    "        message = self.remove_links(message)\n",
    "        message = self.remove_usertags(message)\n",
    "        message = self.remove_hashtags(message)\n",
    "\n",
    "        # 3. we need to get rid of language noise\n",
    "        message = self.remove_punctuation(message)\n",
    "        message = self.remove_stopwords(message)\n",
    "        message = self.lemmatize(message)\n",
    "\n",
    "        # 4. we need tokenize the message\n",
    "        message = self.tokenize(message)\n",
    "\n",
    "        # expose the sanitized message without single characters\n",
    "        return [token for token in message if len(token) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitizer = Sanitizer()\n",
    "\n",
    "# A function that extracts which words exist in a text based on a list of words to which we compare.\n",
    "def word_feats(words):\n",
    "        return dict([(word, True) for word in words])\n",
    "\n",
    "# Get the negative reviews for movies    \n",
    "neg_tweets = [sanitizer.sanitize(tweet) for tweet\n",
    "              in twitter_samples.strings('negative_tweets.json')]\n",
    "\n",
    "# Get the positive reviews for movies\n",
    "pos_tweets = [sanitizer.sanitize(tweet) for tweet \n",
    "              in twitter_samples.strings('positive_tweets.json')]\n",
    "\n",
    "# Find the features that most correspond to negative reviews    \n",
    "negfeats = [(word_feats(f), 'neg') for f in neg_tweets]\n",
    "\n",
    "# Find the features that most correspond to positive reviews\n",
    "posfeats = [(word_feats(f), 'pos') for f in pos_tweets]\n",
    "\n",
    "# We would only use 7500 instances to train on. The quarter of the reviews left is for testing purposes.\n",
    "negcutoff = int(len(negfeats)*3/4)\n",
    "poscutoff = int(len(posfeats)*3/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 7500 instances, test on 2500 instances\n",
      "accuracy: 0.7208\n",
      "Most Informative Features\n",
      "                follower = True              pos : neg    =     26.2 : 1.0\n",
      "                    glad = True              pos : neg    =     25.7 : 1.0\n",
      "                 arrived = True              pos : neg    =     21.8 : 1.0\n",
      "                     sad = True              neg : pos    =     20.3 : 1.0\n",
      "                    sick = True              neg : pos    =     19.7 : 1.0\n",
      "               community = True              pos : neg    =     16.3 : 1.0\n",
      "                      ll = True              neg : pos    =     14.6 : 1.0\n",
      "                    miss = True              neg : pos    =     13.6 : 1.0\n",
      "              definitely = True              pos : neg    =     13.0 : 1.0\n",
      "                     ugh = True              neg : pos    =     13.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Construct the training dataset containing 50% positive reviews and 50% negative reviews\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "\n",
    "# Construct the negative dataset containing 50% positive reviews and 50% negative reviews\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "print ('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train a NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "\n",
    "# Test the trained classifier and display the most informative features.\n",
    "print ('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load in tweets:\n",
    "\n",
    "# # Create pickle of DataFrame of all us tweets with added state locations and mentions\n",
    "# names = ['realDonaldTrump', 'HillaryClinton']\n",
    "\n",
    "# print(\"Number of US country tweets\", twitter_db.count_documents(filter = \n",
    "#  {\"place.country_code\" : \"US\"}))\n",
    "\n",
    "\n",
    "# # Set up pipeline to find tweets mentioning one or both candidates \n",
    "# # and create columns with booleans for the mention of each candidate\n",
    "# pipeline_sen_state = [{\"$match\" : {\"place.country_code\" : \"US\",\n",
    "#                                    \"place.state\":{\"$exists\" : True},\n",
    "#                                    \"sanitized_text\":{\"$exists\" : True},\n",
    "#                                    \"entities.user_mentions.screen_name\":{\"$in\": names}\n",
    "#                                   }},\n",
    "#                       {\"$project\": { \"_id\" : 1, \"id\": 1,\"text\": 1, \"sanitized_text\" :1}}\n",
    "#                         ]\n",
    "# sen_counter = twitter_db.aggregate(pipeline_sen_state)\n",
    "\n",
    "# sen_state_df = pd.DataFrame(sen_counter)\n",
    "# print(\"Number of tweets from the US mentioning one or both candidates: \"+str(\n",
    "#     sen_state_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sen_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize tweet text\n",
    "\n",
    "# sen_state_df[\"sanitized_text\"] = [sanitizer.sanitize(tweet) for tweet \n",
    "#                               in sen_state_df[\"text\"]]\n",
    "\n",
    "# sen_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop empty rows\n",
    "\n",
    "# sen_state_df = sen_state_df[sen_state_df['sanitized_text'].map(lambda d: len(d)) > 0]\n",
    "\n",
    "# sen_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Sanitized text into MongoDB\n",
    "\n",
    "# matched = 0\n",
    "# modified = 0\n",
    "# for tweet in sen_state_df.itertuples():\n",
    "#     result = twitter_db.update_many(\n",
    "#         filter = {\"_id\" : tweet[1]},\n",
    "#         update = {\"$set\": { \"sanitized_text\" : tweet.sanitized_text}}\n",
    "#                                    )\n",
    "#     matched += result.matched_count\n",
    "#     modified += result.modified_count\n",
    "\n",
    "\n",
    "# print(\"Matched: \"+str(matched)+\", Modified: \"+  str(modified))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_san =[{\"$match\":{\"_id\" : sen_state_df[\"_id\"].values[0],\n",
    "#                \"sanitized_text\": {\"$exists\": True }}},\n",
    "#             {\"$project\" :{\"_id\" : 1, \"Feat\": {\"$in\":['collusion',\"$sanitized_text\"]}}}]\n",
    "                       \n",
    "\n",
    "# cursor =twitter_db.aggregate(pipeline_san)\n",
    "# test = list(cursor)\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary measuring word frequencies\n",
    "def get_words_in_tweets(text):\n",
    "    all_words = []\n",
    "    for words in text:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_words_in_tweets_correct(text):\n",
    "    all_words = []\n",
    "    for words, sentiment in text:\n",
    "        all_words.extend(words.keys())\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_words = get_words_in_tweets_correct(trainfeats)\n",
    "# get_words = get_words_in_tweets(sen_state_df[\"sanitized_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: the list of words\n",
    "# Output: the frequency of those words apearing in tweets\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8906"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_word_features = get_word_features(get_words)\n",
    "tweet_word_features_set = set(tweet_word_features)\n",
    "len(tweet_word_features_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_word_features_filtered = dict()\n",
    "# wordlist = nltk.FreqDist(get_words)\n",
    "# for k, v in wordlist.items():\n",
    "#     # Filter out all words that are only once in the data\n",
    "#     if v > 1:\n",
    "#         tweet_word_features_filtered[k] = v\n",
    "\n",
    "# tweet_word_features_filtered = tweet_word_features_filtered.keys()\n",
    "# len(tweet_word_features_filtered)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our features based on which tweets contain which word\n",
    "def extract_features(document):    \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in tweet_word_features:\n",
    "        features[word] = (word in document)\n",
    "    return features\n",
    "\n",
    "def extract_features_fast(document):\n",
    "    document_words = set(document)\n",
    "    features = {word: (word in document_words) for word in  tweet_word_features_set}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19429867499999887\n",
      "0.12867898800000432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test which extractor is faster\n",
    "import timeit\n",
    "print(timeit.timeit('extract_features([\"this\", \"is\", \"a\", \"test\", \"follower\"])',\n",
    "                    \"from __main__ import extract_features\", number=100))\n",
    "print(timeit.timeit('extract_features_fast([\"this\", \"is\", \"a\", \"test\", \"follower\"])',\n",
    "                    \"from __main__ import extract_features_fast\", number=100))\n",
    "\n",
    "# print(timeit.timeit('classifier.classify(extract_features_fast([\"this\", \"is\", \"a\", \"test\", \"follower\"]))',\n",
    "#                     \"from __main__ import extract_features_fast, classifier\", number=100))\n",
    "extract_features([\"this\", \"is\", \"a\", \"test\", \"follower\"]) == extract_features_fast([\"this\", \"is\", \"a\", \"test\", \"follower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del sen_state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra code used to delete field from mondogDB\n",
    "\n",
    "# result = twitter_db.update_many( filter = {\"twitter_sentiment_correct\": {\"$exists\": True }},\n",
    "#                        update = {\"$unset\": { \"twitter_sentiment_correct\" : False }})\n",
    "# matched = result.matched_count\n",
    "# modified = result.modified_count\n",
    "# print(\"Matched: \"+str(matched)+\", Modified: \"+  str(modified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_feat_test =[{\"$match\":{\"twitter_sentiment\":{\"$exists\" : True}}},\n",
    "#                      {\"$project\" :{\"_id\" : 1,\"id\" :1 ,\"state\": \"$place.state\",\n",
    "#                                    \"Mentions_Trump\": {\"$in\": [names[0],\n",
    "#                                                 \"$entities.user_mentions.screen_name\"]},\n",
    "#                                    \"Mentions_Clinton\": {\"$in\": [names[1],\n",
    "#                                                 \"$entities.user_mentions.screen_name\"]},\n",
    "#                                    \"twitter_sentiment\": 1}},\n",
    "#                     {\"$limit\" : 2}]\n",
    "                       \n",
    "\n",
    "# cursor =twitter_db.aggregate(pipeline_feat_test)\n",
    "# test = list(cursor)\n",
    "# for item in test:\n",
    "#     print(item[\"twitter_sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in test:\n",
    "#     print(item[\"features\"])\n",
    "#     print(item[\"id\"],classifier.classify(item[\"features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(sen_state_df[\"_id\"][:2].values[slice(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify tweets\n",
    "\n",
    "# Use mongoDB for low memory use and being able to stop and start\n",
    "\n",
    "# Extract sanitized text and ID of unclassified tweets\n",
    "pipeline_class =[{\"$match\":{\"sanitized_text\": {\"$exists\": True }\n",
    "                            # Find only tweets without a classification label\n",
    "                           ,\"twitter_sentiment_correct\":{\"$exists\" : False}\n",
    "                           }},\n",
    "                 {\"$project\" :{\"_id\" : 1,\"id\" :1 , \"sanitized_text\": 1}}\n",
    "#                  ,{\"$limit\": 10}\n",
    "                ]\n",
    "                       \n",
    "# Create cursur to itterate over\n",
    "cursor_collection =twitter_db.aggregate(pipeline_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:14.896998 seconds\n",
      "Last classification was: pos\n",
      "2000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:00:58.949378 seconds\n",
      "Last classification was: pos\n",
      "3000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:06.724490 seconds\n",
      "Last classification was: neg\n",
      "4000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:04.434036 seconds\n",
      "Last classification was: neg\n",
      "5000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:00:56.443005 seconds\n",
      "Last classification was: neg\n",
      "6000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:00:52.427199 seconds\n",
      "Last classification was: neg\n",
      "7000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:00:53.314005 seconds\n",
      "Last classification was: neg\n",
      "8000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:03.813025 seconds\n",
      "Last classification was: neg\n",
      "9000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:03.493012 seconds\n",
      "Last classification was: pos\n",
      "10000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:15.663068 seconds\n",
      "Last classification was: neg\n",
      "11000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:03.112971 seconds\n",
      "Last classification was: neg\n",
      "12000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:00.249565 seconds\n",
      "Last classification was: pos\n",
      "13000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:00:59.183523 seconds\n",
      "Last classification was: pos\n",
      "14000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:07.060951 seconds\n",
      "Last classification was: pos\n",
      "15000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:02.175710 seconds\n",
      "Last classification was: neg\n",
      "16000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:07.893356 seconds\n",
      "Last classification was: pos\n",
      "17000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:03.327487 seconds\n",
      "Last classification was: pos\n",
      "18000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:00:55.657762 seconds\n",
      "Last classification was: neg\n",
      "19000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:03.211765 seconds\n",
      "Last classification was: pos\n",
      "20000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:00:49.812827 seconds\n",
      "Last classification was: neg\n",
      "21000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:00.318780 seconds\n",
      "Last classification was: pos\n",
      "22000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:05.771029 seconds\n",
      "Last classification was: neg\n",
      "23000  tweets classified!\n",
      "Classification rate is 1000 tweets in 0:01:05.881176 seconds\n",
      "Last classification was: neg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6cfa1d8cf284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     twitter_db.update_one(\n\u001b[1;32m      9\u001b[0m         \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"_id\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"$set\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m\"twitter_sentiment_correct\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     result = (item[\"id\"],classification)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36mupdate_one\u001b[0;34m(self, filter, update, upsert, bypass_document_validation, collation, array_filters, session)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                 \u001b[0mbypass_doc_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbypass_document_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0mcollation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marray_filters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                 session=session),\n\u001b[0m\u001b[1;32m   1003\u001b[0m             write_concern.acknowledged)\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m_update_retryable\u001b[0;34m(self, criteria, document, upsert, check_keys, multi, manipulate, write_concern, op_id, ordered, bypass_doc_val, collation, array_filters, session)\u001b[0m\n\u001b[1;32m    856\u001b[0m         return self.__database.client._retryable_write(\n\u001b[1;32m    857\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mwrite_concern\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_concern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macknowledged\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmulti\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             _update, session)\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m     def replace_one(self, filter, replacement, upsert=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_retryable_write\u001b[0;34m(self, retryable, func, session)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;34m\"\"\"Internal retryable write helper.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tmp_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry_with_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretryable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reset_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_retry_with_session\u001b[0;34m(self, retryable, func, session, bulk)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                             \u001b[0;32mraise\u001b[0m \u001b[0mlast_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                         \u001b[0mretryable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretryable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mServerSelectionTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_retrying\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(session, sock_info, retryable_write)\u001b[0m\n\u001b[1;32m    852\u001b[0m                 \u001b[0mbypass_doc_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbypass_doc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0marray_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marray_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                 retryable_write=retryable_write)\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         return self.__database.client._retryable_write(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, sock_info, criteria, document, upsert, check_keys, multi, manipulate, write_concern, op_id, ordered, bypass_doc_val, collation, array_filters, session, retryable_write)\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             retryable_write=retryable_write).copy()\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0m_check_write_command_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;31m# Add the updatedExisting field for compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;31m# Catch socket.error, KeyboardInterrupt, etc. and close ourselves.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_connection_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_doc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36m_raise_connection_failure\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0m_raise_connection_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields)\u001b[0m\n\u001b[1;32m    582\u001b[0m                            \u001b[0muse_op_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_msg_enabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                            \u001b[0munacknowledged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munacknowledged\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                            user_fields=user_fields)\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOperationFailure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(sock, dbname, spec, slave_ok, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, check_keys, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mresponse_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreceive_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             unpacked_docs = reply.unpack_response(\n\u001b[1;32m    150\u001b[0m                 codec_options=codec_options, user_fields=user_fields)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36mreceive_message\u001b[0;34m(sock, request_id, max_message_size)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Ignore the response's request id.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     length, _, response_to, op_code = _UNPACK_HEADER(\n\u001b[0;32m--> 181\u001b[0;31m         _receive_data_on_socket(sock, 16))\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;31m# No request_id for exhaust cursor \"getMore\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequest_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36m_receive_data_on_socket\u001b[0;34m(sock, length)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mbytes_read\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0mchunk_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes_read\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_errno_from_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number = 0\n",
    "intervaltime=datetime.datetime.now()\n",
    "# t_sample_result = []\n",
    "\n",
    "for item in cursor_collection:\n",
    "    features = extract_features_fast(item[\"sanitized_text\"])\n",
    "    classification = classifier.classify(features)\n",
    "    twitter_db.update_one(\n",
    "        filter = {\"_id\" : item[\"_id\"]},\n",
    "        update = {\"$set\": { \"twitter_sentiment_correct\" : classification}}\n",
    "    )\n",
    "#     result = (item[\"id\"],classification)\n",
    "#     t_sample_result.extend(result)\n",
    "    number+= 1\n",
    "    \n",
    "    if number % 1000 == 0:\n",
    "        print(number,\" tweets classified!\")\n",
    "        print(\"Classification rate is 1000 tweets in %s seconds\" % str(datetime.datetime.now()-intervaltime))\n",
    "        print(\"Last classification was: %s\" % classification)\n",
    "        intervaltime=datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['realDonaldTrump', 'HillaryClinton']\n",
    "pipeline_sen_final = [{\"$match\" : {\"place.country_code\" : \"US\",\n",
    "                                   \"twitter_sentiment_correct\":{\"$exists\" : True},\n",
    "                                   \"entities.user_mentions.screen_name\":{\"$in\": names}\n",
    "                                  }},\n",
    "                      {\"$project\": { \"_id\" : 1, \"id\": 1, \"sentiment\":\"$twitter_sentiment_correct\" ,\n",
    "                                    \"state\": \"$place.state\",\n",
    "                                    \"Mentions_Trump\": {\"$in\": [names[0],\n",
    "                                                \"$entities.user_mentions.screen_name\"]},\n",
    "                                     \"Mentions_Clinton\": {\"$in\": [names[1],\n",
    "                                                \"$entities.user_mentions.screen_name\"]}\n",
    "                                   }}]\n",
    "final_counter = twitter_db.aggregate(pipeline_sen_final)\n",
    "\n",
    "sen_final_df = pd.DataFrame(final_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234247"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count total amount of classified tweets\n",
    "twitter_db.count_documents({\"twitter_sentiment_correct\":{\"$exists\" : True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The tweet we are about to classify\n",
    "# san_text = sen_state_df['sanitized_text'].tolist()\n",
    "\n",
    "# sent_list= [\"\"]*len(sen_state_df['sanitized_text'].tolist())\n",
    "# feat_list = [extract_features(t) for t in san_text[:5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(san_text[0],feat_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number = 0\n",
    "\n",
    "# for feat in feat_list:\n",
    "#     starttime=datetime.datetime.now()\n",
    "#     foo = classifier.classify(feat)\n",
    "#     sent_list[number] = foo\n",
    "#     number+= 1\n",
    "    \n",
    "#     if number % 100 == 0:\n",
    "#         print(number,\" tweets classified!\")\n",
    "#         print(\"Classification rate is 100 tweets in %s seconds\" % str(datetime.datetime.now()-starttime))\n",
    "#         print(\"Last classification was: %s\" % foo)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_df['sentiment'] = pd.Series(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign mentions column to Trump, Clinton or Both\n",
    "series_both = (sen_final_df[\"Mentions_Trump\"] & sen_final_df[\"Mentions_Clinton\"])\n",
    "series_trump = (sen_final_df[\"Mentions_Trump\"] & ~sen_final_df[\"Mentions_Clinton\"])\n",
    "series_clinton = (~sen_final_df[\"Mentions_Trump\"] & sen_final_df[\"Mentions_Clinton\"])\n",
    "sen_final_df[\"Mentions\"] = None\n",
    "print(sum(sen_final_df[\"Mentions\"].isna()))\n",
    "sen_final_df.loc[series_both,\"Mentions\"] = \"both\"\n",
    "sen_final_df.loc[series_trump,\"Mentions\"] = \"trump\"\n",
    "sen_final_df.loc[series_clinton,\"Mentions\"] = \"clinton\"\n",
    "print(sum(sen_final_df[\"Mentions\"].isna()))\n",
    "sen_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_summary_inc_both = sen_final_df[[\"id\",\"state\", \"Mentions\",\"sentiment\"]].groupby(\n",
    "    [\"state\", \"Mentions\",\"sentiment\"]).count()\n",
    "state_summary_twitter = state_summary_inc_both.loc[(slice(None),[\"clinton\", \"trump\"]),:]\n",
    "state_summary_twitter.to_pickle(\"datasets/state_sen_summary_twitter_sample.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_summary_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
