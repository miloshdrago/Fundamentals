{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import twitter_samples\n",
    "import pprint\n",
    "\n",
    "# Load dependencies\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import ConnectionFailure\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Location MongoDB:\n",
    "mongo_host = None\n",
    "# Name collection\n",
    "client_name = \"fundamentals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection\n",
    "try:\n",
    "    client = MongoClient(mongo_host)\n",
    "    client.admin.command('ismaster')\n",
    "    db = client[client_name] \n",
    "    twitter_db = db.twitter\n",
    "    \n",
    "except ConnectionFailure:\n",
    "    print(\"Connection to MongoDB server could not be established\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sanitizer(object):\n",
    "    \"\"\"\n",
    "    Class for sanitizing twitter messages.\n",
    "\n",
    "    @example        ```\n",
    "                    sanitizer = Sanitizer()\n",
    "\n",
    "                    with open('mytextfile.txt', 'r') as f:\n",
    "                        for line in f:\n",
    "                            print(sanitizer.sanitize(line))\n",
    "                    ```\n",
    "\n",
    "    @dependencies   nltk, string, re\n",
    "    @author         Tycho Atsma <tycho.atsma@student.uva.nl>\n",
    "    @file           Sanitizer.py\n",
    "    @documentation  public\n",
    "    @copyright      University of Amsterdam\n",
    "    \"\"\"\n",
    "    punctuation = string.punctuation\n",
    "    stopwords = nltk.corpus.stopwords\n",
    "    Lemmatizer = nltk.stem.WordNetLemmatizer\n",
    "    Tokenizer = nltk.tokenize.TweetTokenizer\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \"\"\"\n",
    "        self.punctuation_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        self.lemmatizer = self.Lemmatizer()\n",
    "        self.tokenizer = self.Tokenizer()\n",
    "\n",
    "    def remove_punctuation(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove punctuation from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        return message.translate(self.punctuation_table).strip()\n",
    "\n",
    "    def remove_stopwords(self, message, language=\"english\"):\n",
    "        \"\"\"\n",
    "        Method to remove stopwords from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @param  string  Language of the stopwords (default: english).\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        stops = self.stopwords.words(language)\n",
    "        tokens = message.split()\n",
    "        return \" \".join([token for token in tokens if token not in stops]).strip()\n",
    "\n",
    "    def remove_links(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove links from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        # source: https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url\n",
    "        pattern = r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n",
    "        return re.sub(pattern, \"\", message).strip()\n",
    "\n",
    "    def remove_usertags(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove usertags from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        pattern = r\"(\\@\\w*)\"\n",
    "        return re.sub(pattern, \"\", message).strip()\n",
    "\n",
    "    def remove_hashtags(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove hashtags from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        pattern = r\"(\\#\\w*)\"\n",
    "        return re.sub(pattern, \"\", message).strip()\n",
    "\n",
    "    def lemmatize(self, message):\n",
    "        \"\"\"\n",
    "        Method to lemmatize a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        tokens = message.split()\n",
    "        return \" \".join([self.lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "    def tokenize(self, message):\n",
    "        \"\"\"\n",
    "        Method to tokenize a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        return self.tokenizer.tokenize(message)\n",
    "\n",
    "    def sanitize(self, message):\n",
    "        \"\"\"\n",
    "        Method to sanitize a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        # 1. we need to normalize the message\n",
    "        message = unicodedata.normalize(\"NFC\", message.lower().encode('utf8').decode('utf8'))\n",
    "\n",
    "        # 2. we need to get rid of specific types of tokens\n",
    "        message = self.remove_links(message)\n",
    "        message = self.remove_usertags(message)\n",
    "        message = self.remove_hashtags(message)\n",
    "\n",
    "        # 3. we need to get rid of language noise\n",
    "        message = self.remove_punctuation(message)\n",
    "        message = self.remove_stopwords(message)\n",
    "        message = self.lemmatize(message)\n",
    "\n",
    "        # 4. we need tokenize the message\n",
    "        message = self.tokenize(message)\n",
    "\n",
    "        # expose the sanitized message without single characters\n",
    "        return [token for token in message if len(token) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitizer = Sanitizer()\n",
    "\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "\n",
    "# A function that extracts which words exist in a text based on a list of words to which we compare.\n",
    "def word_feats(words):\n",
    "        return dict([(word, True) for word in words])\n",
    "\n",
    "# Get the negative reviews for movies    \n",
    "negids = movie_reviews.fileids('neg')\n",
    "\n",
    "# Get the positive reviews for movies\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Find the features that most correspond to negative reviews    \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "\n",
    "# Find the features that most correspond to positive reviews\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# We would only use 7500 instances to train on. The quarter of the reviews left is for testing purposes.\n",
    "negcutoff = int(len(negfeats)*3/4)\n",
    "poscutoff = int(len(posfeats)*3/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Construct the training dataset containing 50% positive reviews and 50% negative reviews\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "\n",
    "# Construct the negative dataset containing 50% positive reviews and 50% negative reviews\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "print ('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train a NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "\n",
    "# Test the trained classifier and display the most informative features.\n",
    "print ('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_tweet_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bf9991046d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtotal_movie_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegfeats_sk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposfeats_sk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmovie_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_movie_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmovie_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_tweet_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'total_tweet_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "def word_feats(words):\n",
    "        return [word for word in words]\n",
    "sanitizer = Sanitizer()\n",
    "\n",
    "# Get the negative reviews for movies    \n",
    "neg_movie_sk = movie_reviews.fileids('neg')\n",
    "\n",
    "# Get the positive reviews for movies\n",
    "pos_movie_sk = movie_reviews.fileids('pos')\n",
    "\n",
    "# Find the features that most correspond to negative reviews    \n",
    "negfeats_sk = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in neg_movie_sk]\n",
    "\n",
    "# Find the features that most correspond to positive reviews\n",
    "posfeats_sk = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in pos_movie_sk]\n",
    "\n",
    "total_movie_dataset = negfeats_sk + posfeats_sk\n",
    "movie_feat = [\" \".join(tweet[0]) for tweet in total_movie_dataset]\n",
    "movie_target = [tweet[1] for tweet in total_tweet_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split dataset into training set and test set\n",
    "# 75% training and 25% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(movie_feat, movie_target, \n",
    "                                                                        test_size=0.25,random_state=109) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "clf = GridSearchCV(text_clf, tuned_parameters, cv=10, scoring='accuracy')\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, clf.predict(x_test), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "filename = 'movies_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = 'movies_model.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test setup\n",
    "\n",
    "# Create a dictionary measuring word frequencies\n",
    "def get_words_in_tweets(text):\n",
    "    all_words = []\n",
    "    for words in text:\n",
    "        all_words.extend(words.split(\" \"))\n",
    "    return all_words\n",
    "\n",
    "def get_words_in_tweets_correct(text):\n",
    "    all_words = []\n",
    "    for words, sentiment in text:\n",
    "        all_words.extend(words.keys())\n",
    "    return all_words\n",
    "\n",
    "# Input: the list of words\n",
    "# Output: the frequency of those words apearing in tweets\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "\n",
    "# Construct our features based on which tweets contain which word\n",
    "def extract_features_fast(document):\n",
    "    document_words = set(document)\n",
    "    features = {word: (word in document_words) for word in  tweet_word_features_set}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39659\n",
      "39768\n"
     ]
    }
   ],
   "source": [
    "# Check if both techniques have the same length vocabulary\n",
    "get_words = get_words_in_tweets(movie_feat)\n",
    "tweet_word_features = get_word_features(get_words)\n",
    "tweet_word_features_set = set(tweet_word_features)\n",
    "\n",
    "vector = CountVectorizer()\n",
    "X = vector.fit_transform(movie_feat)\n",
    "print(len(vector.get_feature_names()))\n",
    "print(len(tweet_word_features_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify tweets\n",
    "\n",
    "# Use mongoDB for low memory use and being able to stop and start\n",
    "\n",
    "# Extract sanitized text and ID of unclassified tweets\n",
    "pipeline_skl =[{\"$match\":{\"sanitized_text\": {\"$exists\": True }\n",
    "                            # Find only tweets without a classification label\n",
    "                           ,\"movie_sentiment_skl\":{\"$exists\" : False}\n",
    "                           }},\n",
    "                 {\"$project\" :{\"_id\" : 1,\"id\" :1 , \"sanitized_text\": 1}}\n",
    "#                  ,{\"$limit\": 10}\n",
    "                ]\n",
    "                       \n",
    "# Create cursur to itterate over\n",
    "cursor_skl =twitter_db.aggregate(pipeline_skl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 0\n",
    "intervaltime=datetime.datetime.now()\n",
    "# t_sample_result = []\n",
    "\n",
    "for item in cursor_skl:\n",
    "    classification = clf.predict([\" \".join(item[\"sanitized_text\"])])\n",
    "    \n",
    "    twitter_db.update_one(\n",
    "        filter = {\"_id\" : item[\"_id\"]},\n",
    "        update = {\"$set\": { \"movie_sentiment_skl\" : classification[0]}}\n",
    "    )\n",
    "#     result = (item[\"id\"],classification)\n",
    "#     t_sample_result.extend(result)\n",
    "    number+= 1\n",
    "    \n",
    "    if number % 10000 == 0:\n",
    "        print(number,\" tweets classified!\")\n",
    "        print(\"Classification rate is 10.000 tweets in %s seconds\" % str(datetime.datetime.now()-intervaltime))\n",
    "        print(\"Last classification was: %s\" % classification)\n",
    "        intervaltime=datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347276"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count total amount of classified tweets\n",
    "twitter_db.count_documents({\"movie_sentiment_skl\":{\"$exists\" : True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test equallity\n",
    "# names = ['realDonaldTrump', 'HillaryClinton']\n",
    "# pipeline_test_eq = [{\"$match\" : {\"place.country_code\" : \"US\",\n",
    "#                                  \"twitter_sentiment_correct\":{\"$exists\" : True},\n",
    "#                                  \"twitter_sentiment_skl\":{\"$exists\" : True},\n",
    "#                                  \"entities.user_mentions.screen_name\":{\"$in\": names}\n",
    "#                                   }},\n",
    "#                     {\"$project\": { \"_id\" : 1, \"id\": 1, \"sentiment_nltk\":\"$twitter_sentiment_correct\" ,\n",
    "#                                     \"sentiment_skl\":\"$twitter_sentiment_skl\"\n",
    "#                                    }},\n",
    "#                     {\"$limit\" : 100000}\n",
    "#                    ]\n",
    "# test_counter = twitter_db.aggregate(pipeline_test_eq)\n",
    "# test_df = pd.DataFrame(test_counter)\n",
    "\n",
    "# print(test_df.shape)\n",
    "# print(test_df[test_df[\"sentiment_skl\"]  ==  test_df[\"sentiment_nltk\"]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['realDonaldTrump', 'HillaryClinton']\n",
    "pipeline_sen_final = [{\"$match\" : {\"place.country_code\" : \"US\",\n",
    "                                   \"movie_sentiment_skl\":{\"$exists\" : True},\n",
    "                                   \"entities.user_mentions.screen_name\":{\"$in\": names}\n",
    "                                  }},\n",
    "                      {\"$project\": { \"_id\" : 1, \"id\": 1, \"sentiment\":\"$movie_sentiment_skl\" ,\n",
    "                                    \"state\": \"$place.state\",\n",
    "                                    \"Mentions_Trump\": {\"$in\": [names[0],\n",
    "                                                \"$entities.user_mentions.screen_name\"]},\n",
    "                                     \"Mentions_Clinton\": {\"$in\": [names[1],\n",
    "                                                \"$entities.user_mentions.screen_name\"]}\n",
    "                                   }}]\n",
    "final_counter = twitter_db.aggregate(pipeline_sen_final)\n",
    "\n",
    "sen_final_df = pd.DataFrame(final_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347276\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>state</th>\n",
       "      <th>Mentions_Trump</th>\n",
       "      <th>Mentions_Clinton</th>\n",
       "      <th>Mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5d7f499f98cd5b074bafb1a2</td>\n",
       "      <td>764039733076897792</td>\n",
       "      <td>pos</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5d7f499f98cd5b074bafb1a6</td>\n",
       "      <td>764039917924069376</td>\n",
       "      <td>neg</td>\n",
       "      <td>California</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5d7f499f98cd5b074bafb1a8</td>\n",
       "      <td>764039926161604608</td>\n",
       "      <td>pos</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5d7f499f98cd5b074bafb1a9</td>\n",
       "      <td>764039928116240384</td>\n",
       "      <td>neg</td>\n",
       "      <td>Texas</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5d7f499f98cd5b074bafb1ab</td>\n",
       "      <td>764039948567576576</td>\n",
       "      <td>pos</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                  id sentiment       state  \\\n",
       "0  5d7f499f98cd5b074bafb1a2  764039733076897792       pos   Louisiana   \n",
       "1  5d7f499f98cd5b074bafb1a6  764039917924069376       neg  California   \n",
       "2  5d7f499f98cd5b074bafb1a8  764039926161604608       pos  New Jersey   \n",
       "3  5d7f499f98cd5b074bafb1a9  764039928116240384       neg       Texas   \n",
       "4  5d7f499f98cd5b074bafb1ab  764039948567576576       pos    Maryland   \n",
       "\n",
       "   Mentions_Trump  Mentions_Clinton Mentions  \n",
       "0            True             False    trump  \n",
       "1            True             False    trump  \n",
       "2            True             False    trump  \n",
       "3           False              True  clinton  \n",
       "4            True             False    trump  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign mentions column to Trump, Clinton or Both\n",
    "series_both = (sen_final_df[\"Mentions_Trump\"] & sen_final_df[\"Mentions_Clinton\"])\n",
    "series_trump = (sen_final_df[\"Mentions_Trump\"] & ~sen_final_df[\"Mentions_Clinton\"])\n",
    "series_clinton = (~sen_final_df[\"Mentions_Trump\"] & sen_final_df[\"Mentions_Clinton\"])\n",
    "sen_final_df[\"Mentions\"] = None\n",
    "print(sum(sen_final_df[\"Mentions\"].isna()))\n",
    "sen_final_df.loc[series_both,\"Mentions\"] = \"both\"\n",
    "sen_final_df.loc[series_trump,\"Mentions\"] = \"trump\"\n",
    "sen_final_df.loc[series_clinton,\"Mentions\"] = \"clinton\"\n",
    "print(sum(sen_final_df[\"Mentions\"].isna()))\n",
    "sen_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_summary_inc_both = sen_final_df[[\"id\",\"state\", \"Mentions\",\"sentiment\"]].groupby(\n",
    "    [\"state\", \"Mentions\",\"sentiment\"]).count()\n",
    "state_summary_twitter = state_summary_inc_both.loc[(slice(None),[\"clinton\", \"trump\"]),:]\n",
    "state_summary_twitter.to_pickle(\"../datasets/state_sen_summary_movie_skl.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_summary_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347276\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Mentions_Trump</th>\n",
       "      <th>Mentions_Clinton</th>\n",
       "      <th>Mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>764039733076897792</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>764039917924069376</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>764039926161604608</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>764039928116240384</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>764039948567576576</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  Mentions_Trump  Mentions_Clinton Mentions\n",
       "0  764039733076897792            True             False    trump\n",
       "1  764039917924069376            True             False    trump\n",
       "2  764039926161604608            True             False    trump\n",
       "3  764039928116240384           False              True  clinton\n",
       "4  764039948567576576            True             False    trump"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Andrew\n",
    "names = ['realDonaldTrump', 'HillaryClinton']\n",
    "pipeline_sen_final = [{\"$match\" : {\"place.country_code\" : \"US\",\n",
    "                                   \"movie_sentiment_skl\":{\"$exists\" : True},\n",
    "                                   \"entities.user_mentions.screen_name\":{\"$in\": names}\n",
    "                                  }},\n",
    "                      {\"$project\": { \"_id\" : 0, \"id\": 1,\n",
    "                                    \"Mentions_Trump\": {\"$in\": [names[0],\n",
    "                                                \"$entities.user_mentions.screen_name\"]},\n",
    "                                     \"Mentions_Clinton\": {\"$in\": [names[1],\n",
    "                                                \"$entities.user_mentions.screen_name\"]}\n",
    "                                   }}]\n",
    "final_counter = twitter_db.aggregate(pipeline_sen_final)\n",
    "\n",
    "sen_final_df = pd.DataFrame(final_counter)\n",
    "\n",
    "# Assign mentions column to Trump, Clinton or Both\n",
    "series_both = (sen_final_df[\"Mentions_Trump\"] & sen_final_df[\"Mentions_Clinton\"])\n",
    "series_trump = (sen_final_df[\"Mentions_Trump\"] & ~sen_final_df[\"Mentions_Clinton\"])\n",
    "series_clinton = (~sen_final_df[\"Mentions_Trump\"] & sen_final_df[\"Mentions_Clinton\"])\n",
    "sen_final_df[\"Mentions\"] = None\n",
    "print(sum(sen_final_df[\"Mentions\"].isna()))\n",
    "sen_final_df.loc[series_both,\"Mentions\"] = \"both\"\n",
    "sen_final_df.loc[series_trump,\"Mentions\"] = \"trump\"\n",
    "sen_final_df.loc[series_clinton,\"Mentions\"] = \"clinton\"\n",
    "print(sum(sen_final_df[\"Mentions\"].isna()))\n",
    "\n",
    "\n",
    "print(sen_final_df.shape)\n",
    "sen_final_df.to_pickle(\"../datasets/id-mentions-andrew.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347276, 4)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
