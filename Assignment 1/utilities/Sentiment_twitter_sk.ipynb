{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import twitter_samples\n",
    "import pprint\n",
    "\n",
    "# Load dependencies\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import ConnectionFailure\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Location MongoDB:\n",
    "mongo_host = None\n",
    "# Name collection\n",
    "client_name = \"fundamentals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection\n",
    "try:\n",
    "    client = MongoClient(mongo_host)\n",
    "    client.admin.command('ismaster')\n",
    "    db = client[client_name] \n",
    "    twitter_db = db.twitter\n",
    "    \n",
    "except ConnectionFailure:\n",
    "    print(\"Connection to MongoDB server could not be established\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sanitizer(object):\n",
    "    \"\"\"\n",
    "    Class for sanitizing twitter messages.\n",
    "\n",
    "    @example        ```\n",
    "                    sanitizer = Sanitizer()\n",
    "\n",
    "                    with open('mytextfile.txt', 'r') as f:\n",
    "                        for line in f:\n",
    "                            print(sanitizer.sanitize(line))\n",
    "                    ```\n",
    "\n",
    "    @dependencies   nltk, string, re\n",
    "    @author         Tycho Atsma <tycho.atsma@student.uva.nl>\n",
    "    @file           Sanitizer.py\n",
    "    @documentation  public\n",
    "    @copyright      University of Amsterdam\n",
    "    \"\"\"\n",
    "    punctuation = string.punctuation\n",
    "    stopwords = nltk.corpus.stopwords\n",
    "    Lemmatizer = nltk.stem.WordNetLemmatizer\n",
    "    Tokenizer = nltk.tokenize.TweetTokenizer\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \"\"\"\n",
    "        self.punctuation_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        self.lemmatizer = self.Lemmatizer()\n",
    "        self.tokenizer = self.Tokenizer()\n",
    "\n",
    "    def remove_punctuation(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove punctuation from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        return message.translate(self.punctuation_table).strip()\n",
    "\n",
    "    def remove_stopwords(self, message, language=\"english\"):\n",
    "        \"\"\"\n",
    "        Method to remove stopwords from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @param  string  Language of the stopwords (default: english).\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        stops = self.stopwords.words(language)\n",
    "        tokens = message.split()\n",
    "        return \" \".join([token for token in tokens if token not in stops]).strip()\n",
    "\n",
    "    def remove_links(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove links from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        # source: https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url\n",
    "        pattern = r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n",
    "        return re.sub(pattern, \"\", message).strip()\n",
    "\n",
    "    def remove_usertags(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove usertags from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        pattern = r\"(\\@\\w*)\"\n",
    "        return re.sub(pattern, \"\", message).strip()\n",
    "\n",
    "    def remove_hashtags(self, message):\n",
    "        \"\"\"\n",
    "        Method to remove hashtags from a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        pattern = r\"(\\#\\w*)\"\n",
    "        return re.sub(pattern, \"\", message).strip()\n",
    "\n",
    "    def lemmatize(self, message):\n",
    "        \"\"\"\n",
    "        Method to lemmatize a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        tokens = message.split()\n",
    "        return \" \".join([self.lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "    def tokenize(self, message):\n",
    "        \"\"\"\n",
    "        Method to tokenize a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        return self.tokenizer.tokenize(message)\n",
    "\n",
    "    def sanitize(self, message):\n",
    "        \"\"\"\n",
    "        Method to sanitize a twitter message.\n",
    "        @param  string  Twitter message.\n",
    "        @return string\n",
    "        \"\"\"\n",
    "        # 1. we need to normalize the message\n",
    "        message = unicodedata.normalize(\"NFC\", message.lower().encode('utf8').decode('utf8'))\n",
    "\n",
    "        # 2. we need to get rid of specific types of tokens\n",
    "        message = self.remove_links(message)\n",
    "        message = self.remove_usertags(message)\n",
    "        message = self.remove_hashtags(message)\n",
    "\n",
    "        # 3. we need to get rid of language noise\n",
    "        message = self.remove_punctuation(message)\n",
    "        message = self.remove_stopwords(message)\n",
    "        message = self.lemmatize(message)\n",
    "\n",
    "        # 4. we need tokenize the message\n",
    "        message = self.tokenize(message)\n",
    "\n",
    "        # expose the sanitized message without single characters\n",
    "        return [token for token in message if len(token) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitizer = Sanitizer()\n",
    "\n",
    "# A function that extracts which words exist in a text based on a list of words to which we compare.\n",
    "def word_feats(words):\n",
    "        return dict([(word, True) for word in words])\n",
    "\n",
    "# Get the negative reviews for movies    \n",
    "neg_tweets = [sanitizer.sanitize(tweet) for tweet\n",
    "              in twitter_samples.strings('negative_tweets.json')]\n",
    "\n",
    "# Get the positive reviews for movies\n",
    "pos_tweets = [sanitizer.sanitize(tweet) for tweet \n",
    "              in twitter_samples.strings('positive_tweets.json')]\n",
    "\n",
    "# Find the features that most correspond to negative reviews    \n",
    "negfeats = [(word_feats(f), 'neg') for f in neg_tweets]\n",
    "\n",
    "# Find the features that most correspond to positive reviews\n",
    "posfeats = [(word_feats(f), 'pos') for f in pos_tweets]\n",
    "\n",
    "# We would only use 7500 instances to train on. The quarter of the reviews left is for testing purposes.\n",
    "negcutoff = int(len(negfeats)*3/4)\n",
    "poscutoff = int(len(posfeats)*3/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 7500 instances, test on 2500 instances\n",
      "accuracy: 0.7208\n",
      "Most Informative Features\n",
      "                follower = True              pos : neg    =     26.2 : 1.0\n",
      "                    glad = True              pos : neg    =     25.7 : 1.0\n",
      "                 arrived = True              pos : neg    =     21.8 : 1.0\n",
      "                     sad = True              neg : pos    =     20.3 : 1.0\n",
      "                    sick = True              neg : pos    =     19.7 : 1.0\n",
      "               community = True              pos : neg    =     16.3 : 1.0\n",
      "                      ll = True              neg : pos    =     14.6 : 1.0\n",
      "                    miss = True              neg : pos    =     13.6 : 1.0\n",
      "                      aw = True              neg : pos    =     13.0 : 1.0\n",
      "                     ugh = True              neg : pos    =     13.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Construct the training dataset containing 50% positive reviews and 50% negative reviews\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "\n",
    "# Construct the negative dataset containing 50% positive reviews and 50% negative reviews\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "print ('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train a NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "\n",
    "# Test the trained classifier and display the most informative features.\n",
    "print ('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_feats(words):\n",
    "        return [word for word in words]\n",
    "\n",
    "# Get the negative reviews for movies    \n",
    "neg_tweets_sk = [sanitizer.sanitize(tweet) for tweet\n",
    "              in twitter_samples.strings('negative_tweets.json')]\n",
    "\n",
    "# Get the positive reviews for movies\n",
    "pos_tweets_sk = [sanitizer.sanitize(tweet) for tweet \n",
    "              in twitter_samples.strings('positive_tweets.json')]\n",
    "\n",
    "# Find the features that most correspond to negative reviews    \n",
    "negfeats_sk = [(word_feats(f), 'neg') for f in neg_tweets_sk]\n",
    "\n",
    "# Find the features that most correspond to positive reviews\n",
    "posfeats_sk = [(word_feats(f), 'pos') for f in pos_tweets_sk]\n",
    "\n",
    "total_tweet_dataset = negfeats_sk + posfeats_sk\n",
    "tweets_feat = [\" \".join(tweet[0]) for tweet in total_tweet_dataset]\n",
    "tweets_target = [tweet[1] for tweet in total_tweet_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split dataset into training set and test set\n",
    "# 75% training and 25% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweets_feat, tweets_target, \n",
    "                                                                        test_size=0.25,random_state=109) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg     0.7267    0.7961    0.7598      1236\n",
      "         pos     0.7801    0.7073    0.7419      1264\n",
      "\n",
      "    accuracy                         0.7512      2500\n",
      "   macro avg     0.7534    0.7517    0.7509      2500\n",
      "weighted avg     0.7537    0.7512    0.7508      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "clf = GridSearchCV(text_clf, tuned_parameters, cv=10, scoring='accuracy')\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, clf.predict(x_test), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test setup\n",
    "\n",
    "# Create a dictionary measuring word frequencies\n",
    "def get_words_in_tweets(text):\n",
    "    all_words = []\n",
    "    for words in text:\n",
    "        all_words.extend(words.split(\" \"))\n",
    "    return all_words\n",
    "\n",
    "def get_words_in_tweets_correct(text):\n",
    "    all_words = []\n",
    "    for words, sentiment in text:\n",
    "        all_words.extend(words.keys())\n",
    "    return all_words\n",
    "\n",
    "# Input: the list of words\n",
    "# Output: the frequency of those words apearing in tweets\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "\n",
    "# Construct our features based on which tweets contain which word\n",
    "def extract_features_fast(document):\n",
    "    document_words = set(document)\n",
    "    features = {word: (word in document_words) for word in  tweet_word_features_set}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if both techniques have the same length vocabulary\n",
    "get_words = get_words_in_tweets(tweets_feat)\n",
    "tweet_word_features = get_word_features(get_words)\n",
    "tweet_word_features_set = set(tweet_word_features)\n",
    "\n",
    "vector = CountVectorizer()\n",
    "X = vector.fit_transform(tweets_feat)\n",
    "len(vector.get_feature_names())\n",
    "len(tweet_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify tweets\n",
    "\n",
    "# Use mongoDB for low memory use and being able to stop and start\n",
    "\n",
    "# Extract sanitized text and ID of unclassified tweets\n",
    "pipeline_skl =[{\"$match\":{\"sanitized_text\": {\"$exists\": True }\n",
    "                            # Find only tweets without a classification label\n",
    "                           ,\"twitter_sentiment_skl\":{\"$exists\" : False}\n",
    "                           }},\n",
    "                 {\"$project\" :{\"_id\" : 1,\"id\" :1 , \"sanitized_text\": 1}}\n",
    "#                  ,{\"$limit\": 10}\n",
    "                ]\n",
    "                       \n",
    "# Create cursur to itterate over\n",
    "cursor_skl =twitter_db.aggregate(pipeline_skl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 0\n",
    "intervaltime=datetime.datetime.now()\n",
    "# t_sample_result = []\n",
    "\n",
    "for item in cursor_skl:\n",
    "    classification = clf.predict([\" \".join(item[\"sanitized_text\"])])\n",
    "    \n",
    "    twitter_db.update_one(\n",
    "        filter = {\"_id\" : item[\"_id\"]},\n",
    "        update = {\"$set\": { \"twitter_sentiment_skl\" : classification[0]}}\n",
    "    )\n",
    "#     result = (item[\"id\"],classification)\n",
    "#     t_sample_result.extend(result)\n",
    "    number+= 1\n",
    "    \n",
    "    if number % 10000 == 0:\n",
    "        print(number,\" tweets classified!\")\n",
    "        print(\"Classification rate is 10.000 tweets in %s seconds\" % str(datetime.datetime.now()-intervaltime))\n",
    "        print(\"Last classification was: %s\" % classification)\n",
    "        intervaltime=datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347276"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count total amount of classified tweets\n",
    "twitter_db.count_documents({\"twitter_sentiment_skl\":{\"$exists\" : True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347276, 4)\n",
      "(191367, 4)\n"
     ]
    }
   ],
   "source": [
    "# Test equallity\n",
    "names = ['realDonaldTrump', 'HillaryClinton']\n",
    "pipeline_test_eq = [{\"$match\" : {\"place.country_code\" : \"US\",\n",
    "                                 \"movie_sentiment_skl\":{\"$exists\" : True},\n",
    "                                 \"twitter_sentiment_skl\":{\"$exists\" : True},\n",
    "                                 \"entities.user_mentions.screen_name\":{\"$in\": names}\n",
    "                                  }},\n",
    "                    {\"$project\": { \"_id\" : 1, \"id\": 1, \"sentiment_movies\":\"$movie_sentiment_skl\" ,\n",
    "                                    \"sentiment_tweet\":\"$twitter_sentiment_skl\"\n",
    "                                   }},\n",
    "#                     {\"$limit\" : 0}\n",
    "                   ]\n",
    "test_counter = twitter_db.aggregate(pipeline_test_eq)\n",
    "test_df = pd.DataFrame(test_counter)\n",
    "\n",
    "print(test_df.shape)\n",
    "print(test_df[test_df[\"sentiment_tweet\"]  ==  test_df[\"sentiment_movies\"]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['realDonaldTrump', 'HillaryClinton']\n",
    "pipeline_sen_final = [{\"$match\" : {\"place.country_code\" : \"US\",\n",
    "                                   \"twitter_sentiment_skl\":{\"$exists\" : True},\n",
    "                                   \"entities.user_mentions.screen_name\":{\"$in\": names}\n",
    "                                  }},\n",
    "                      {\"$project\": { \"_id\" : 1, \"id\": 1, \"sentiment\":\"$twitter_sentiment_skl\" ,\n",
    "                                    \"state\": \"$place.state\",\n",
    "                                    \"Mentions_Trump\": {\"$in\": [names[0],\n",
    "                                                \"$entities.user_mentions.screen_name\"]},\n",
    "                                     \"Mentions_Clinton\": {\"$in\": [names[1],\n",
    "                                                \"$entities.user_mentions.screen_name\"]}\n",
    "                                   }}]\n",
    "final_counter = twitter_db.aggregate(pipeline_sen_final)\n",
    "\n",
    "sen_final_df = pd.DataFrame(final_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347276\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>state</th>\n",
       "      <th>Mentions_Trump</th>\n",
       "      <th>Mentions_Clinton</th>\n",
       "      <th>Mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5d7f499f98cd5b074bafb1a2</td>\n",
       "      <td>764039733076897792</td>\n",
       "      <td>neg</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5d7f499f98cd5b074bafb1a6</td>\n",
       "      <td>764039917924069376</td>\n",
       "      <td>neg</td>\n",
       "      <td>California</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5d7f499f98cd5b074bafb1a8</td>\n",
       "      <td>764039926161604608</td>\n",
       "      <td>neg</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5d7f499f98cd5b074bafb1a9</td>\n",
       "      <td>764039928116240384</td>\n",
       "      <td>neg</td>\n",
       "      <td>Texas</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5d7f499f98cd5b074bafb1ab</td>\n",
       "      <td>764039948567576576</td>\n",
       "      <td>pos</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                  id sentiment       state  \\\n",
       "0  5d7f499f98cd5b074bafb1a2  764039733076897792       neg   Louisiana   \n",
       "1  5d7f499f98cd5b074bafb1a6  764039917924069376       neg  California   \n",
       "2  5d7f499f98cd5b074bafb1a8  764039926161604608       neg  New Jersey   \n",
       "3  5d7f499f98cd5b074bafb1a9  764039928116240384       neg       Texas   \n",
       "4  5d7f499f98cd5b074bafb1ab  764039948567576576       pos    Maryland   \n",
       "\n",
       "   Mentions_Trump  Mentions_Clinton Mentions  \n",
       "0            True             False    trump  \n",
       "1            True             False    trump  \n",
       "2            True             False    trump  \n",
       "3           False              True  clinton  \n",
       "4            True             False    trump  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign mentions column to Trump, Clinton or Both\n",
    "series_both = (sen_final_df[\"Mentions_Trump\"] & sen_final_df[\"Mentions_Clinton\"])\n",
    "series_trump = (sen_final_df[\"Mentions_Trump\"] & ~sen_final_df[\"Mentions_Clinton\"])\n",
    "series_clinton = (~sen_final_df[\"Mentions_Trump\"] & sen_final_df[\"Mentions_Clinton\"])\n",
    "sen_final_df[\"Mentions\"] = None\n",
    "print(sum(sen_final_df[\"Mentions\"].isna()))\n",
    "sen_final_df.loc[series_both,\"Mentions\"] = \"both\"\n",
    "sen_final_df.loc[series_trump,\"Mentions\"] = \"trump\"\n",
    "sen_final_df.loc[series_clinton,\"Mentions\"] = \"clinton\"\n",
    "print(sum(sen_final_df[\"Mentions\"].isna()))\n",
    "sen_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_summary_inc_both = sen_final_df[[\"id\",\"state\", \"Mentions\",\"sentiment\"]].groupby(\n",
    "    [\"state\", \"Mentions\",\"sentiment\"]).count()\n",
    "state_summary_twitter = state_summary_inc_both.loc[(slice(None),[\"clinton\", \"trump\"]),:]\n",
    "state_summary_twitter.to_pickle(\"datasets/state_sen_summary_twitter_skl.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"4\" valign=\"top\">Alabama</td>\n",
       "      <td rowspan=\"2\" valign=\"top\">clinton</td>\n",
       "      <td>neg</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pos</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">trump</td>\n",
       "      <td>neg</td>\n",
       "      <td>1921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pos</td>\n",
       "      <td>2112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Alaska</td>\n",
       "      <td>clinton</td>\n",
       "      <td>neg</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id\n",
       "state   Mentions sentiment      \n",
       "Alabama clinton  neg         526\n",
       "                 pos         420\n",
       "        trump    neg        1921\n",
       "                 pos        2112\n",
       "Alaska  clinton  neg         111"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_summary_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
